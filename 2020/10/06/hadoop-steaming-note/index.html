<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Hadoop Sreaming Note · woodpile27</title><meta name="description" content="Hadoop Sreaming Note - John Doe"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://woodpile27.cn/atom.xml" title="woodpile27"><meta name="generator" content="Hexo 5.0.2"><link rel="alternate" href="/atom.xml" title="woodpile27" type="application/atom+xml">
</head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://github.com/woodpile27" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">Hadoop Sreaming Note</h1><div class="post-info">Oct 6, 2020</div><div class="post-content"><h3 id="配置参数"><a href="#配置参数" class="headerlink" title="配置参数"></a>配置参数</h3><ul>
<li><strong>mapreduce.job.name</strong>：job name。</li>
<li><strong>mapreduce.job.priority</strong>：优先级调度，只是在计算任务向集群服务申请资源的时候会起作用。</li>
<li><strong>mapreduce.job.running.map.limit</strong>：每个作业最多可以同时跑多少个map，默认是0，当值为0或负数时无限制。</li>
<li><strong>mapreduce.job.reduces</strong>：reduce任务个数，默认是1。</li>
<li><strong>mapred.input.dir.error.pass</strong>：跳过输入路径错误。</li>
<li><strong>mapred.split.zero.file.skip</strong>：如果文件长度为0则划分split。</li>
</ul>
<h4 id="容错"><a href="#容错" class="headerlink" title="容错"></a>容错</h4><ul>
<li><strong>mapreduce.map.maxattempts</strong>：每个map任务最大重试次数，重试次数超过该值则认为任务运行失败。默认是4。</li>
<li><strong>mapreduce.reduce.maxattempts</strong>：每个reduce任务最大重试次数，同上。  </li>
<li><strong>mapreduce.task.timeout</strong>：Task超时时间。如果一个任务在一定时间内不读取新的数据，也没有输出数据，则认为其处于block状态。为了防止用户程序永远block住不退出，强制设置了一个超时时间（单位毫秒），默认是300000。</li>
</ul>
<h4 id="压缩"><a href="#压缩" class="headerlink" title="压缩"></a>压缩</h4><ul>
<li><strong>mapreduce.map.output.compress</strong>：在通过网络发送之前map的输出是否压缩。默认false。</li>
<li><strong>mapreduce.map.output.compress.codec</strong>：指定map输出所用的压缩codec。</li>
</ul>
<p>对map输出进行数据压缩可以加速网络传输，对reduce输出进行数据压缩可以减少磁盘空间，如果输入文件是压缩的,那么在根据文件扩展名推断出相应的codec后，MapReduce会在读取文件是自动解压缩文件。</p>
<h4 id="效率和稳定性"><a href="#效率和稳定性" class="headerlink" title="效率和稳定性"></a>效率和稳定性</h4><ul>
<li><strong>mapreduce.map.speculative</strong>：map开启推测执行。默认true。</li>
<li><strong>mapreduce.reduce.speculative</strong>：reduce开启推测执行。默认true。</li>
</ul>
<p>在作业执行时，由于一些原因，比如硬件老化、软件层面的不恰当配置等。某些机器上的任务实例执行的很慢，拖慢了整个作业的进度。Hadoop不会尝试去诊断或者修复这些慢任务，相反它会在集群的其他节点上去启动这些慢任务的多个示例作为备份，这就是hadoop的推测执行（speculative execution）。</p>
<h3 id="map数量如何确定"><a href="#map数量如何确定" class="headerlink" title="map数量如何确定"></a>map数量如何确定</h3><p>map的个数等于split的个数。<br>其由三个因素决定：</p>
<ol>
<li>输入文件数目</li>
<li>输入文件大小</li>
<li>配置参数</li>
</ol>
<p>一般来说，对于每一个输入的文件会有一个map split。如果输入文件太大，会把大文件划分成多个split进行处理，每个map处理一个split。<br>涉及的参数：</p>
<ul>
<li><strong>mapreduce.input.fileinputformat.split.minsize</strong>：启动map最小的split size大小，默认0</li>
<li><strong>mapreduce.input.fileinputformat.split.maxsize</strong>：启动map最大的split size大小，默认256M</li>
<li><strong>dfs.block.size</strong>：block块大小。</li>
</ul>
<p>计算公式：<strong>splitSize =  Math.max(minSize, Math.min(maxSize, blockSize))</strong></p>
<p>FileInputFormat class 的getSplits()的伪代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">num_splits &#x3D; 0</span><br><span class="line">for each input file f:</span><br><span class="line">    remaining &#x3D; f.length</span><br><span class="line">    while remaining &#x2F; split_size &gt; split_slope:</span><br><span class="line">        num_splits +&#x3D; 1</span><br><span class="line">        remaining -&#x3D; split_size</span><br><span class="line">    where:</span><br><span class="line">        split_slope &#x3D; 1.1 分割斜率</span><br><span class="line">        split_size &#x3D;~ dfs.blocksize 分割大小约等于hdfs块大小</span><br></pre></td></tr></table></figure>
<p>会有一个比例进行运算来进行切片，为了减少资源的浪费<br>例如一个文件大小为260M，在进行MapReduce运算时，会首先使用260M/128M(blocksize)，得出的结果和1.1进行比较，大于则切分出一个128M作为一个分片，剩余132M，再次除以128(blocksize)，得到结果为1.03，小于1.1则将132作为一个切片，即最终260M被切分为两个切片进行处理，而非3个切片。  </p>
<h3 id="传参"><a href="#传参" class="headerlink" title="传参"></a>传参</h3><h4 id="sys-argv"><a href="#sys-argv" class="headerlink" title="sys.argv"></a>sys.argv</h4><p>mapper：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line">arg1 = sys.argv[<span class="number">1</span>]</span><br><span class="line">arg2 = sys.argv[<span class="number">2</span>]</span><br></pre></td></tr></table></figure>
<p>hadoop straming：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$HADOOP</span> jar <span class="variable">$streaming_path</span> \</span><br><span class="line">    -mapper <span class="string">&quot;mapper.py arg1 arg2&quot;</span> \</span><br><span class="line">    -file <span class="string">&quot;mapper.py&quot;</span></span><br></pre></td></tr></table></figure>
<h4 id="环境变量"><a href="#环境变量" class="headerlink" title="环境变量"></a>环境变量</h4><p>mapper：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os  </span><br><span class="line"></span><br><span class="line">date_start = os.environ.get(<span class="string">&quot;date_start&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>hadoop straming：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$HADOOP</span> jar <span class="variable">$streaming_path</span> \</span><br><span class="line">    -mapper <span class="string">&quot;mapper.py&quot;</span> \</span><br><span class="line">    -cmdenv <span class="string">&quot;date_start=<span class="variable">$date</span>&quot;</span></span><br></pre></td></tr></table></figure>
<h3 id="设置多个输入目录"><a href="#设置多个输入目录" class="headerlink" title="设置多个输入目录"></a>设置多个输入目录</h3><h4 id="使用多个-input选项-or-逗号分割"><a href="#使用多个-input选项-or-逗号分割" class="headerlink" title="使用多个-input选项 or 逗号分割"></a>使用多个-input选项 or 逗号分割</h4><p>多个 -input：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$HADOOP</span> jar <span class="variable">$streaming_path</span> \</span><br><span class="line">    -input <span class="string">&quot;/user/foo/dir1&quot;</span> \</span><br><span class="line">    -input <span class="string">&quot;/user/foo/dir2&quot;</span></span><br></pre></td></tr></table></figure>
<p>逗号分割：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$HADOOP</span> jar <span class="variable">$streaming_path</span> \</span><br><span class="line">    -input <span class="string">&quot;/user/foo/dir1,/user/foo/dir2&quot;</span></span><br></pre></td></tr></table></figure>
<h4 id="通配符"><a href="#通配符" class="headerlink" title="通配符"></a>通配符</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$HADOOP</span> jar <span class="variable">$streaming_path</span> \</span><br><span class="line">    -input <span class="string">&quot;/user/foo/dir[1-2]&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="variable">$HADOOP</span> jar <span class="variable">$streaming_path</span> \</span><br><span class="line">    -input <span class="string">&quot;/user/foo/&#123;dir1,dir2&#125;&quot;</span></span><br></pre></td></tr></table></figure>
<h3 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">input_file=(<span class="string">&quot;/user/foo/dir1&quot;</span> <span class="string">&quot;/user/foo/dir2&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="variable">$HADOOP</span> jar <span class="variable">$streaming_path</span> \</span><br><span class="line">    -input <span class="variable">$&#123;input_file[@]&#125;</span></span><br></pre></td></tr></table></figure>

</div></article></div></main><footer><div class="paginator"><a href="/2020/09/16/docker-monitor/" class="next">NEXT</a></div><div class="copyright"><p>© 2015 - 2020 <a href="http://woodpile27.cn">John Doe</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script></body></html>